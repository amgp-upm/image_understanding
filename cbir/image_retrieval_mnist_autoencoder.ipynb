{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-based image retrieval by autoencoders\n",
    "\n",
    "**Content-based image retrieval** (CBIR) is the mechanism of retrieving images relevant to a given query from a large collection of images known as an image database, based on their semantic or visual content rather than on derived attributes or keyword descriptors prescriptively defined for them.\n",
    "\n",
    "As shown in the picture there are some basic steps involved in query and retrieval:\n",
    "\n",
    "- Feature extraction: Of course, this part involves the extraction of image characteristics, such as texture, color, etc. It could also be considered a preprocessing step to, for example, resize or improve the quality of the images.\n",
    "- Similarity measure: The similarity measurement is used to estimate the query image with the database images by similarity. The dissimilarity between the feature vector of the query image and the database images is calculated using different distance metrics. The higher the dissimilarity, the less similar the two images are. Some commonly used distances are: Euclidean distance, block distance, Minkowski distance and Mahalanobis distance.\n",
    "- Retrieve the results: The N most similar images are displayed to the user.\n",
    "\n",
    "![CBIR](../images/cbir_01.jpg)\n",
    "\n",
    "\n",
    "This notebook is based on the work published by Rupapara et al.[<sup>1</sup>](#fn1). In here special neural network called autoencoder is used for extracting the features which are going to use for extracting image features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# Images come as vectors, so it is necessary to reshape them.\n",
    "x_train = np.reshape(x_train, (len(x_train), input_shape[0], \n",
    "                               input_shape[1], input_shape[2]))\n",
    "x_test = np.reshape(x_test, (len(x_test), input_shape[0], \n",
    "                             input_shape[1], input_shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction phase: autoencoder\n",
    "\n",
    "An autoencoder is a neural network that is unsupervised which means that doesn't require any labeled data. \n",
    "\n",
    "They work by compressing the input into a latent space representation and reconstructing the output from this representation:\n",
    "\n",
    "- Encoder: the part of the network that compresses the input into a latent space  representation (i.e., representation of compressed data). It can be represented by an encoding function \\\\( h=f(x) \\\\).\n",
    "- Decoder: This part aims to reconstruct the input from the latent space representation. It can be represented by a decoding function \\\\( r=g(h) \\\\).\n",
    "\n",
    "![CBIR Autoencoder](../images/cbir_02.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the autoencoder network\n",
    "\n",
    "def autoencoder_model(input_shape=(28, 28, 1)):\n",
    "    \"\"\" Autoencoder model \"\"\"\n",
    "    input_img = Input(shape=input_shape)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "    encoded = MaxPooling2D((2, 2), padding='same', name='encoder')(x)\n",
    "\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(input_shape[2], (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "autoencoder = autoencoder_model()\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder for denoising\n",
    "\n",
    "Instead of training the autoencoder to learn an original image and reconstruct it, let's train it, so it can denoise an image. To do this, we modify the input adding some noise will maintain the original image as target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding noise to images\n",
    "\n",
    "noise_factor = 0.05\n",
    "\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(\n",
    "    loc=0.0, scale=1.0, size=x_train.shape)\n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(\n",
    "    loc=0.0, scale=1.0, size=x_test.shape)\n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "for idx in range(3):\n",
    "    y = autoencoder.predict(np.expand_dims(x_train_noisy[idx, ...], axis=0))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(9, 3), sharex='col', sharey='row')\n",
    "    ax[0].imshow(np.squeeze(x_train[idx, ...]), cmap='gray')\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[1].imshow(np.squeeze(x_train_noisy[idx, ...]), cmap='gray')\n",
    "    ax[1].set_title('Noisy Image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the autoencoder for 10 epochs.\n",
    "\n",
    "It takes about 15 minutes in CPU, only because it is a smal network an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(5)\n",
    "\n",
    "model_name = 'autoencoder_mnist.h5'\n",
    "\n",
    "if not os.path.exists(model_name):\n",
    "    autoencoder = autoencoder_model()\n",
    "    autoencoder.fit(x_train, x_train_noisy,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_test, x_test_noisy))\n",
    "\n",
    "    autoencoder.save(model_name)\n",
    "else:\n",
    "    autoencoder = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of the result of the denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5):\n",
    "    y = autoencoder.predict(np.expand_dims(x_train_noisy[idx, ...], axis=0))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(9, 3), sharex='col', sharey='row')\n",
    "    ax[0].imshow(np.squeeze(x_train[idx, ...]), cmap='gray')\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[1].imshow(np.squeeze(x_train_noisy[idx, ...]), cmap='gray')\n",
    "    ax[1].set_title('Noisy Image')\n",
    "    ax[2].imshow(np.squeeze(y), cmap='gray')\n",
    "    ax[2].set_title('Denoised Image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the autoencoder for feature extraction (latent space features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain features from middle layer\n",
    "autoencoder = load_model(model_name)\n",
    "encoder = Model(inputs=autoencoder.input, \n",
    "                outputs=autoencoder.get_layer('encoder').output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the database \n",
    "\n",
    "Creation of the database by extracting the autoencoder features for all images in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from train data\n",
    "codes_name = 'learned_codes_mnist.npy'\n",
    "\n",
    "if not os.path.exists(codes_name):\n",
    "    learned_codes = encoder.predict(x_train)\n",
    "    # The sise of the latent space is (None, 4, 4, 8), so we have to vectorize it.\n",
    "    learned_codes = learned_codes.reshape(learned_codes.shape[0],\n",
    "                                          learned_codes.shape[1] * \n",
    "                                          learned_codes.shape[2] * \n",
    "                                          learned_codes.shape[3])\n",
    "    np.save(codes_name, learned_codes)\n",
    "else:\n",
    "    learned_codes = np.load(codes_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval phase\n",
    "\n",
    "We retrieve the most similar images from 9 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_retrieve_images = 9\n",
    "np.random.seed(5)\n",
    "    \n",
    "indexes = np.random.randint(0, x_test.shape[0], 8) # Select 8 random images for testing\n",
    "\n",
    "for idx in indexes:\n",
    "    # Extract features from test image\n",
    "    im_test = x_test[idx]\n",
    "    test_code = encoder.predict(np.expand_dims(im_test, axis=0))\n",
    "    test_code = test_code.reshape(test_code.shape[0], test_code.shape[1] * \n",
    "                                  test_code.shape[2] * test_code.shape[3])\n",
    "    \n",
    "    # Calculate the distance from\n",
    "    distances = []\n",
    "    for code in learned_codes:\n",
    "        distance = np.sqrt(np.sum(np.square(code - test_code))) # Euclidean norm\n",
    "        distances.append(distance)\n",
    "    distances = np.array(distances)\n",
    "    \n",
    "    # Ordering the images according to similarity\n",
    "    sorted_idx = distances.argsort()\n",
    "    images_idx = sorted_idx[:num_retrieve_images]\n",
    "    \n",
    "    \n",
    "    # Showing the results\n",
    "    fig, axes = plt.subplots(int((num_retrieve_images + 1)/5 + 0.5), 5,\n",
    "                           figsize=(15, 5), sharex='col', sharey='row')\n",
    "    \n",
    "    fig.suptitle('Example {}: {}'.format(idx + 1, y_test[idx]), fontsize='x-large')\n",
    "    for ax_idx, ax in enumerate(axes.flat):\n",
    "        if ax_idx == 0:\n",
    "            ax.imshow(np.squeeze(im_test), cmap='gray')\n",
    "            ax.set_title('Test Image')\n",
    "        else:\n",
    "            ax.imshow(np.squeeze(x_train[images_idx[ax_idx - 1]]), cmap='gray')\n",
    "            ax.set_title('Similar Image {}'.format(ax_idx))\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. <span id=\"fn1\">Rupapara, Vaibhav, et al. \"Auto-Encoders for Content-based Image Retrieval with its Implementation Using Handwritten Dataset.\" 2020 5th International Conference on Communication and Electronics Systems (ICCES). IEEE, 2020.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
